<!DOCTYPE html>
<!--[if lt IE 7]><html class="lt-ie9 lt-ie8 lt-ie7" lang="en" dir="ltr"><![endif]-->
<!--[if IE 7]><html class="lt-ie9 lt-ie8" lang="en" dir="ltr"><![endif]-->
<!--[if IE 8]><html class="lt-ie9" lang="en" dir="ltr"><![endif]-->
<!--[if gt IE 8]><!--><html lang="en" dir="ltr"><!--<![endif]-->
<head>
<!--[if IE]><![endif]-->
<!--<meta charset="utf-8" />
<link rel="shortcut icon" href="https://meaningness.com/sites/meaningness.com/files/favicon.ico" type="image/vnd.microsoft.icon" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="alternate" type="application/rss+xml" title="Comments on Meaningness" href="https://meaningness.com/comments-rss.xml" />
<link rel="alternate" type="application/rss+xml" title="Meaningness" href="https://meaningness.com/rss.xml" />
<meta name="description" content="Improving artificial intelligence research with scientific testing, design practice, and meta-rational choice of methods and criteria" />
<meta property="og:site_name" content="Meaningness" />
<meta property="og:type" content="article" />
<meta property="og:title" content="How should we evaluate progress in AI?" />
<meta property="og:url" content="https://meaningness.com/metablog/artificial-intelligence-progress" />
<meta property="og:description" content="Improving artificial intelligence research with scientific testing, design practice, and meta-rational choice of methods and criteria" />
<meta property="og:updated_time" content="2018-07-09T23:29:29-07:00" />
<meta property="og:image" content="https://meaningness.com/images/mn/Wolpertinger_560x569.jpg" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:creator" content="@meaningness" />
<meta name="twitter:title" content="How should we evaluate progress in AI?" />
<meta name="twitter:description" content="Improving artificial intelligence research with scientific testing, design practice, and meta-rational choice of methods and criteria" />
<meta name="twitter:image" content="https://meaningness.com/images/mn/Wolpertinger_560x569.jpg" />
<meta property="article:published_time" content="2018-06-30T22:04:05-07:00" />
<meta property="article:modified_time" content="2018-07-09T23:29:29-07:00" />
-->
<title>Computational Neuroscience</title>

<link type="text/css" rel="stylesheet" href="https://use.typekit.net/ufl4pts.css" media="all" />
<link type="text/css" rel="stylesheet" href="https://meaningness.com/sites/meaningness.com/files/advagg_css/css__Djiw-Yv_iTiqCoXb4Gk1pSf88HP13SDRQifv38hz1mo__H0f3sbci-dPEdDJKEP4ObngKAF6MPJfvMsyqIet7M9o__FK12eMg8Czxsn0tcDiqIatH0T9TgZK4i0-Bg90RD42A.css" media="all" />

<!--[if (lt IE 9)&(!IEMobile 7)]>
<link type="text/css" rel="stylesheet" href="https://meaningness.com/sites/meaningness.com/files/advagg_css/css__1uP48pwjnQkhXNkcki_HzdGl8hL2NTpijyK3flszeUM__zPnD6Q8qshJCAKBiAyv_5RptOHHpztLHJIwTDtf0Fe4__FK12eMg8Czxsn0tcDiqIatH0T9TgZK4i0-Bg90RD42A.css" media="screen" />
<![endif]-->
<!--<script src="https://meaningness.com/sites/meaningness.com/files/advagg_js/js__Q0pewlf3S8JQD4oFAxT1detilXE_AIGuop1DWxQIRlI__A2hr82Db0sVjeaYF85W3YuB27vR75uAUNsya20Q4Ix8__FK12eMg8Czxsn0tcDiqIatH0T9TgZK4i0-Bg90RD42A.js"></script>
<script src="https://meaningness.com/sites/meaningness.com/files/advagg_js/js__Z6gJapvNkOhtzp3DpseBgBiwx3_Thk_rEpvkkAWqwP4__JYnfq9y9JKGsfvcn7ugn8U5zJ6hi8-0DRCJflNkBzxY__FK12eMg8Czxsn0tcDiqIatH0T9TgZK4i0-Bg90RD42A.js" defer="defer" async="async"></script>
<script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();})(window,document,"script","https://meaningness.com/sites/meaningness.com/files/googleanalytics/analytics.js?pq5017","ga");ga("create", "UA-1156013-8", {"cookieDomain":"auto"});ga("send", "pageview");</script>
<script>jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"meaningness","theme_token":"0rgzS0F6U0wK_5ave2HK9LS0qITCTrzjM1ONchBI42Q","css":{"https:\/\/use.typekit.net\/ufl4pts.css":1,"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/aggregator\/aggregator.css":1,"modules\/comment\/comment.css":1,"modules\/field\/theme\/field.css":1,"sites\/all\/modules\/footnotes\/footnotes.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"modules\/book\/book.css":1,"sites\/all\/modules\/views\/css\/views.css":1,"sites\/all\/modules\/comment_notify\/comment_notify.css":1,"sites\/all\/modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/typogrify\/typogrify.css":1,"sites\/all\/themes\/adaptivetheme\/at_core\/css\/at.layout.css":1,"sites\/meaningness.com\/themes\/meaningness\/css\/global.base.css":1,"sites\/meaningness.com\/themes\/meaningness\/css\/global.styles.css":1,"sites\/meaningness.com\/themes\/meaningness\/css\/customization.css":1,"public:\/\/adaptivetheme\/meaningness_files\/meaningness.default.layout.css":1,"sites\/meaningness.com\/themes\/meaningness\/css\/lt-ie9.css":1},"js":{"sites\/all\/modules\/jquery_update\/replace\/jquery\/1.10\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/macros\/macros.js":1,"sites\/all\/modules\/simple_glossary\/simple_glossary.js":1,"sites\/all\/modules\/cleantalk\/src\/js\/apbct-public.js":1,"sites\/all\/modules\/google_analytics\/googleanalytics.js":1,"sites\/meaningness.com\/files\/googleanalytics\/analytics.js":1}},"googleanalytics":{"trackOutbound":1,"trackMailto":1,"trackDownload":1,"trackDownloadExtensions":"7z|aac|arc|arj|asf|asx|avi|bin|csv|doc(x|m)?|dot(x|m)?|exe|flv|gif|gz|gzip|hqx|jar|jpe?g|js|mp(2|3|4|e?g)|mov(ie)?|msi|msp|pdf|phps|png|ppt(x|m)?|pot(x|m)?|pps(x|m)?|ppam|sld(x|m)?|thmx|qtm?|ra(m|r)?|sea|sit|tar|tgz|torrent|txt|wav|wma|wmv|wpd|xls(x|m|b)?|xlt(x|m)|xlam|xml|z|zip"},"urlIsAjaxTrusted":{"\/search\/node":true,"\/metablog\/artificial-intelligence-progress":true},"adaptivetheme":{"meaningness":{"layout_settings":{"bigscreen":"two-sidebars-right","tablet_landscape":"two-sidebars-right-stack","tablet_portrait":"one-col-stack","smalltouch_landscape":"one-col-stack","smalltouch_portrait":"one-col-stack"},"media_query_settings":{"bigscreen":"only screen and (min-width:960px)","tablet_landscape":"only screen and (min-width:9999px) and (max-width:1px)","tablet_portrait":"only screen and (min-width:481px) and (max-width:960px)","smalltouch_landscape":"only screen and (min-width:321px) and (max-width:580px)","smalltouch_portrait":"only screen and (max-width:320px)"}}}});</script>-->
<!--[if lt IE 9]>
<script src="https://meaningness.com/sites/all/themes/adaptivetheme/at_core/scripts/html5.js?pq5017"></script>
<![endif]-->
</head>
<body class="html not-front not-logged-in no-sidebars page-node page-node- page-node-246 node-type-metablog-post site-name-hidden atr-7.x-3.x atv-7.x-3.2">
  <div id="skip-link" class="nocontent">
    <a href="#main-content" class="element-invisible element-focusable">Skip to main content</a>
  </div>
    <div id="page-wrapper">
  <div id="page" class="page">

    <header id="header" class="clearfix" role="banner">
      <div id="logo">
	<img class="site-logo" src="images/banner.jpg" alt="Computational Neuroscience" />      </div>
    </header>

          <h1 id="page-title">Computational Neuroscience: principles guiding our lives (perceptions, thoughts, behaviors...)</h1>
    
    
    
    
    
    <div id="content-wrapper"><div class="container">
      <div id="columns"><div class="columns-inner clearfix">
        <div id="content-column"><div class="content-inner">

          
          <section id="main-content">

            
                          <header id="main-content-header" class="clearfix">

                
              </header>
            
                          <div id="content">
                <div id="block-system-main" class="block block-system no-title odd first last block-count-1 block-region-content block-main" >  
    
  <article id="node-246" class="node node-metablog-post  article clearfix" role="article">
  
  
  
  <div class="node-content">
    <div class="field field-name-body field-type-text-with-summary field-label-hidden view-mode-full"><div class="field-items"><div class="field-item even">
      <!--<figure class="ctrimg"><img src="https://meaningness.com/images/mn/Wolpertinger_560x569.jpg" width="560" height="569" alt="Wolpertinger" title="Wolpertinger" /><figcaption class="ctrimgcaption" markdown="1">Wolpertinger image <a href="https://en.wikipedia.org/wiki/Wolpertinger#/media/File:Wolpertinger.jpg">courtesy</a> Rainer Zenz</figcaption></figure>
      -->

<p>
<hr></hr>
First, I want to give credit and point to a resource I've very recently stumbled upon which dives deep into a wide range of philosophy pertaining to meaning: <a href="https://meaningness.com/" target="_blank">Meaningness</a> (by David Chapman, a retired intellectual who did AI research at MIT)<br><br>

I've only just begun to dip into David Chapman's enjoyable writings but I believe they are well-worth the time. For the sake of putting together the following narrative in a timely manner I've borrowed and tweaked his simple and effective visual formatting style.<br><br>
I also want to thank my professor Greg Conradi Smith for stoking the fires of our ongoing learning journeys. 
<hr></hr>
</p>
<p class="intro_para"></p>
<p>Today it's common to hear ideas thrown around about how the brain <em>processes information</em> like a computer or electrical circuits, but for centuries this same kind thinking lead us to believe that the brain was some kind of mechanical or hydraulic machine. The field of neuroscience has only recently debunked these century old assumptions of how the brain physically functions (Hodgkin & Huxley), but it still lacks a clear understanding of how our nervous system does the seemingly magical things it does, like producing the rich conscious experience we enjoy today. Is there a correct principled way to approach these challenges of how our minds do the things they do with scientific understanding? Many prominent neuroscientists believe so, but any agreement beyond the simple biology of two neuronal connections is scarce.</p>

<p>
It feels intuitive to think of our nervous system in terms of <em>sensory inputs</em>, complex <em>internal processing</em>, and <em>motor outputs</em>, and I don't think this simplified thinking is completely wrong, but I also don't think it's very useful. It doesn't help us understand what's going on inside that black box of "internal processing" to produce the complexity in thought we share. A proper evaluation of the undelying principles can greatly subserve artificial intelligence and medical research, as well as our own philosophical views. Because of this, I think the drive for progress may happen regardless of our individual efforts - which is both a comforting and terrifying idea to some. If the progress of society is out of our individual control - we need to at least seriously consider and spread awareness of the ethics tied to this progress.</p>

<p>
What follows is subjective observations, ideas, and arguments that I have collected and organized into an interesting quilt which attempts to explain our minds to the best of our limited ability. Maintaining a healthy balance of skepticism and curiosity is part of what makes us so good at expanding our collective knowledge.
</p>

<p>
Outline:<br/>
1. Emergence, Entropy, Evolution<br />
2. Simulations, Computation, AI<br />
3. Existential Threats - delicate eddys on Earth
</p>

<h2 id="science">Current Understanding and Observations</h2>
<h3>Emergence</h3>
<blockquote><p>It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.  -Albert Einstein</p></blockquote>


<p>
I want to begin this journey with an observation of the world that has existed for a long time called <em>emergence</em>. This is the idea that simple components can come together and interact with eachother in some way to produce a kind of more complex social whole. The whole is something else than the sum of its parts, and often produces regularities that we can reliably study - which is why our scientific understanding is generally stratified at different discrete levels of abstraction (physics, chemistry, biology, psychology, etc). These are the patterns in the universe that most readily jump out at us. (Kurzgesagt, 2017)
</p>

<p>
  <figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/16W7c0mb-rE" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>
</p>

<p>
In pursuit of a unified <em>theory of everything</em> physicists struggle to connect the two theories on which all modern physics relies on: Quantum Field Theory (the laws of the small) with General Relativity (the laws of the large). Somewhere between these scales, we exist to study them in addition to the many other fields we attempt to connect like physics with chemistry, and chemistry with biology. Perhaps the most contested and challenging connection is that between the mental mind and physical body.</p>

<p>What makes this so hard? Is the difficulty of bridging the gaps between some levels simply because it cannot be done, even in principle?</p>

<p>Descarte's dualism held this view to the extreme: where the mind is a separate entity that controls the physical body. The mind and body clearly interact but this view holds that an explanation of the mental in terms of the physical was simply impossible. This view supports the idea of <em>strong emergence</em>, where the whole system may be completely <em>irreducible</em> to it's component parts. (O'Connor et al., 2015)
</p>

<p>
On the other extreme is Laplace's determinism, which held the idea that if a demon knew the precise location and momentum of every atom in the universe, then in principle it could calculate all of the past and future. This view rejects an idea like <em>strong emergence</em> because the causation is entirely bottom-up, and everything can be reduced in terms of it's component parts. Emergent patterns may then just be a weaker form of emergence, where despite being possible (in principle) to reduce things into their component parts, it is much easier and more pragmatic to directly study those emerging patterns that exist. (Hawking, 1999)
</p>

<p>I found <a href="https://www.closertotruth.com/series/what-strong-emergence" target="_blank">these interviews on the topic</a> (especially the last 2 videos) particularly interesting.
</p>

<p>Philosophers of Science, Tim Maudlin and Barry Loewer seem to lean towards the principle ideas of Laplace with the qualification that our current understanding of the fundamental physical laws of the universe are currently very primitive. They appear to believe Laplace's demon may be possible <em>in principle</em> but not practical or even feasible for humans to accomplish by studying something like biology solely in terms of fundamental physical interactions.

Tim Maudlin points out that ideas in these "higher level" sciences often originate in <em>functional models</em> like Mendelian Genetics and Hebbian Learning, preceding the discovery of their physical manifestations like the corresponding discoveries of DNA and LTP. He says that physics is the only field that can't say "that's not my department" when asked for an explanation that involves any matter in motion, and this is where the need for a theory of everything gets placed squarely on physicists. This also wouldn't necessarily imply that our lives and behaviors are deterministic as Laplace believed because we don't even know for sure if the universe is deterministic or probabilistic due to our lack of a full comprehension of the fundamental physics.</p>

<p>Something I found especially compelling was that Barry Loewer points to Boltzmann's statistical mechanics as an important piece towards a theory of everything. Statistical mechanics explains how taking the probability distribution over all the possible micro-states of the universe is compatible with the macro-state we observe. This has been used to explain how thermodynamic behaviors like temperature and pressure relate to the ensemble activity of microscopic particle fluctuations. I find this to be promising support that many naturally emergent properties can - at least in principle - be explained in terms of their interacting parts.</p>

<h3>Entropy</h3>
<p>Boltzmann's work has been expanded and refined by others to explain the second law of thermodynamics: the entropy, or disorder, of a closed system will never decrease over time - instead it tends to increase until reaching a maximal state at thermodynamic equilibrium. Barry Loewer believes that these kinds of physical processes unfolding in one irreversible direction based on probability distributions could even account for the appearance of <em>time</em> flowing in a single direction.</p>

<p>
These ideas have helped us explain the emergence of temperature in a jar of gas, but can they explain the emergence of life on Earth? Many scientists seem to believe so with the addition of a positive feedback loop created by Darwin's evolution by natural selection:
</p>

<figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/_uAJY1mqtw4" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>
<figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/GcfLZSL7YGw" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>

<h3>Evolution</h3>
<p>
Darwin's ideas are clearly implicit in these scientists' views because the algorithm of evolution by natural selection has built a strong explanatory bridge between the incredibly rare possibility that replicating systems first emerged and the subsequent rapid development in life's complexity. If we remain skeptical, we should ask ourselves if this is an oversimplification. Are small genetic mutations accumulating over time truly sufficient to account for the rapid development of complex evolutionary innovations like eyes, wings, and complex nervous system organization? Some prominent scientists and philosophers think there are important gaps here to be addressed.
</p>

<p>
A few resources which I have yet the chance to explore deeply include Daniel Dennet's <em>Darwin's Dangerous Idea</em>, Kirshner & Gerhart's <em>The Plausibility of Life: Resolving Darwin's Dilemma</em>, and Thomas Nagel's <em>Mind & Cosmos</em>. The premises of these books get at different issues with current evolutionary theory, but for now I want to focus on Nagel's ideas which I have found to be especially important to the discussion.
</p>

<p>
Nagel is a prominent advocate of the idea that the emergence of consciousness, subjective experience, and even life, cannot be reduced or explained by our current understanding of physics. One of his most famous pieces: <em>What is it like to Be a Bat</em> gets at the idea that there is an abstract feeling that comes with being a bat, and we are limited in our ability to fully comprehend that subjective feeling. Tim Maudlin made a similar point that physics is tasked with explaining all of matter in motion, and consciousness feelings are not matter in motion.
</p>

<p>
The "feeling" of pain cannot be described by the physical interactions of your pain receptors, the "blueness" of blue cannot be accounted for by the physical properties of the light waves or the transduction mechanisms of the eye. I feel these are very strong cases for us to treat subjective experience as an especially challenging level of phenomenon which we are far from fully understanding, but I'm not fully convinced that the mental could never be understood in terms of the physical.
</p>

<p>
What would it mean for physicalism to be able to explain subjective experience? I'm sure it will require much more research, but a few functional ideas already exist that I put slightly more faith in than accepting the idea that there's no possible physical explanation. 
</p>

<p>
Marvin Minsky's book <em>The Society of Mind</em> tries to conceptually describe the mind as emerging from the collective activity of individual 'thinking units' of the nervous system. This idea is at least conceptually plausible to me as our collective society appears to have a 'conscious mind' of its own that results from our individual activities. This doesn't really get us much further though, it just reaffirms our original idea of emergence. 
</p>
<p>
Parts of our own bodies are varyingly conscious and physical damage or disorders can affect our cognitive function in a seemingly endless variety of ways. I think this at the very least establishes that consciousness is tied in some way to our physical states. Part of the subjective aspect may result from internal physical representations and models of the world that exist in complex and subjective feedback loops.
</p>

<p>
The functional idea of loops comes from Douglas Hofstadter in his books <em>Gödel, Escher, Bach</em> and <em>I Am a Strange Loop</em>. I find the idea attractive that the appearance of downward causality is only an illusion resulting from the complex cyclic activity in our nervous system. I strongly feel this could provide insight into so many different complex cognitive processes like language, mathematical reasoning, and theory of mind, but our ability to study such activity in humans has ethical and technical limitations.
</p>

<p>
If emergent behavior like consciousness relied on top-down influence (or strong emergence) then it would be impossible to simulate on a machine because we couldn't reduce it to a physical representation. So can we just plug computational theories into a computer and test them out? If we look at the work of some AI researchers, we can see where the issues lie.
</p>

<h2 id="science">Computations, Algorithms, & Simulations</h2>
<blockquote><p>What magical trick makes us intelligent? The trick is that there is no trick. The power of intelligence stems from our vast diversity, not from any single, perfect principle.   -Marvin Minsky</p></blockquote>

<p>
I feel that simulations and toy models can be fun, engaging, and consequently powerful learning tools to experiment with. Because of this, I aim to make this a place for experimenting with simple educational simulations in the near future. The problem is that they also require immense expertise backing them if they are aiming to be realistic or explanatory in any reasonable way.
</p>
<p>
Simulating a brain is not feasible, <a href="http://openworm.org/" target="_blank">researchers are struggling to even simulate the exhaustively studied round worm with just 302 neurons.</a> Even simulating a single cell is a computational challenge without some level of reductionist abstraction. Additionally, without also simulating the right external environment this wouldn't make much sense at all. If we follow Marvin Minsky and Douglas Hofstadter then there's a deeply suggestive intuition in our scientific understanding towards a bottom-up emergence of living systems and subsequent complex intelligent life. So can't we just simulate entropy and evolution to test the emergence of artificial life or AI? 
</p>
<p>
Even if we had full understanding of the underlying physics, it would never be computationally feasible to model all of the physical interactions of our environment and calculate the results like Laplace's demon. Extracting meaning and information in the patterns of the noisy universe is functionally important. Our own energy efficient nervous systems prove to us this is possible, and this may be why our machines increasingly tend towards resembling ourselves.
</p>
<p>
Most attempts at conceptual simulations using genetic algorithms, reinforcement learning, or 'Good Old Fashioned AI,' point out the severe flaws and gaps we have in our knowledge of real intelligence. These are all issues of "design" limitations. (Chapman, 2018) We're trying to play gods with powers of "intelligent design" without a true knowledge of the fundamental principles that should guide a simulation. So if this is true, what's with all the current AI hype? 
</p>
<p>
We may never be able to "upload" our consciousness or create sentient machines - but our current AI's can still solve a handful of computationally interesting problems. Computer vision and natural language processing have seen suprising progress, but it's important to recognize that these deep learning developments are mostly only good at <em>recognizing patterns</em> in data. Recognizing patterns is an important first step which has found tons of use cases, but it says nothing about true computational understanding or logical reasoning. (Chapman, 2018)
</p>
<p>
Karl Friston, a pretentious but widely cited neuroscientist, believes he's come to a global theory of the brain that is so simple and powerful that it can explain the organizing principles of all living systems, which has also been regarded as holding the key to true AI. Unsurprisingly, it's named the free energy principle:
</p>
<figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/NIu_dJGyIQI" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>
<p>
I think there's no real shortage of innovation, so I don't want to dwell too long on the technical problems for now, but I think Minsky makes a good case for the idea that we're nowhere near understanding the seemingly convoluted and complex organization of our own nervous systems. I feel it is even more important to discuss how our technological advancements are shaping the world we live in. Innovation has radically changed the way we live, work, and interact with our environment, and further advancements appear unavoidable. This raises plenty of ethical considerations, but I specifically want to focus on the existential threats we present to the planet, because I think there are lessons to take away from generally intelligent life, and we can all do a little more to spread advocacy for the future of our species.
</p>
<h2 id="science">Global Existential Concerns</h2>
<blockquote><p>Our approach to existential risks cannot be one of trial-and-error. There is no opportunity to learn from errors. The reactive approach — see what happens, limit damages, and learn from experience — is unworkable. Rather, we must take a proactive approach. This requires foresight to anticipate new types of threats and a willingness to take decisive preventive action and to bear the costs (moral and economic) of such actions. -Nick Bostrom</p></blockquote>
<p>
Selfishness and unregulated growth come at an external cost. Among our cells this looks like cancer, in sociology and economics this looks like the tragedy of the commons, in global life... this looks like 'general intelligence.' Existential risk philosophers like Nick Bostrom
 believe any sufficiently powerful general intelligence with unbounded goals, will also pursue ubounded "convergent instrumental goals." These are goals like self-preservation or resource acquisition that could generally help them achieve a wide variety of other goals. For example, if we built a generally intelligent agent who's objective was to simply collect stamps, it may do all sorts of unintended things to achieve this goal like stealing, murdering, or enslaving others for its cause. To me, it's much more concerning how often humans do this to themselves.</p>

<p>
For some people, accumulating money is a convergent instrumental goal because it can be used to achieve a wide variety of terminal goals (you can buy almost anything with enough money). As a result we see all kinds of unintended consequences in society that look not too different from the stamp-collector situation. This in conjuntion with our current global economic competitions can make us short-sighted in our decision-making. It then becomes very challenging to deal with long-term sustainability issues like climate change and inequality. 
</p>
<p>
For now, there is hope that humanity will course-correct as problems become less abstract and we begin to prioritize collective action. Suggestions in neuroscience include renaming climate change to climate crisis and sharing narratives that are more likely to stick in our minds than abstract reality of global temperatures and sea-levels rising. But this begs the question that has been called the Fermi-paradox, where despite the high probability estimates that life may exist on other planets, we've found none.
</p>

<p>
The idea of a Great Filter has been 
</p>
<p>
Post-humanist ideas are widely distributed and some are rather frightening. 
</p>

<figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/MBRqu0YOH14" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>

<p>
  Personally, optimisitc nihilism sounds too much like hakuna matata, and could even be causing us more trouble than necessary as David Chapman suggests. Until we are truly out of options, we should at least try to save our delicate planet in the best ways we can: with our collective action.
</p>
<p>
  Life generally cares about propogating itself - should we use our intelligence to propogate non-human life in space
</p>

<ul class="footnotes">
  <h2 id="science">References:</h2>
  <li>Minsky, ML 1988, Society of Mind, Simon & Schuster, New York.</li>
  <li>Abbott, R 2006, ‘Emergence explained: Abstractions: Getting epiphenomena to do real work’, Wiley Periodicals, Inc., vol. 12, no. 1, pp. 13-26. Available from: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cplx.20146. [1 May 2019].</li>
  <li>Glasgow, RDV 2018, ‘Minimal Selfhood and the Origins of Consciousness’, Würzburg University Press. Available from: https://www.academia.edu/37069229/Minimal_Selfhood_and_the_Origins_of_Consciousness. [1 May 2019].</li>
  <li>Nagel, T 1997, ‘What Is It Like to Be a Bat?’, The Philosophical Review, vol. 83, no. 4, pp. 435-450. Available from: JSTOR. [1 May 2019].</li>
  <li>Hofstadter, DR 1979, Gödel, Escher, Bach: An Eternal Golden Braid, Basic Books, New York.</li>
  <li>Hofstadter, DR 2007,  I Am a Strange Loop, Basic Books, New York.</li>
  <li>Kurzgesagt - In a Nutshell 2017, Emergence - How Stupid Things Become Smart Together, YouTube video, 16 Nov. Available from:https://www.youtube.com/watch?time_continue=2&v=16W7c0mb-rE. [1 May 2017].</li>
  <li>It’s Okay To Be Smart 2018, Where Did Life Come From? (feat. PBS Space Time and Eons!), YouTube Video, 11 Apr. Available from:https://www.youtube.com/watch?v=_uAJY1mqtw4. [1 May 2019].</li>
  <li>PBS Space Time, The Physics of Life (ft. It's Okay to be Smart & PBS Eons!) | Space Time, YouTube video, 11 Apr. Available from: https://www.youtube.com/watch?v=GcfLZSL7YGw. [1 May 2019].</li>
  <li>Chapman, D 2018, ‘How should we evaluate progress in AI?’ Meaningness Metablog, blog post, 30 June. Available from: https://meaningness.com/metablog/artificial-intelligence-progress. [1 May 2019]. </li>
  <li>Bostrom, N 2012, ‘The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents’, Minds and Machines, vol.22, no.2, pp.71-85. Available from:https://link.springer.com/article/10.1007%2Fs11023-012-9281-3. [1 May 2019].</li>
  <li>Serious Science, Free Energy Principle - Karl Friston YouTube video, 16 Jun. Available from: https://www.youtube.com/watch?v=NIu_dJGyIQI. [1 May 2019]. </li>
  <li>Amodei, D, Olah, C, Steinhardt, J, Christiano, P, Schulman, J, & Mané, D 2016, ‘Concrete problems in AI safety’, arXiv preprint arXiv:1606.06565. Available from: https://arxiv.org/pdf/1606.06565.pdf. [1 May 2019].</li>
  <li>Friston, K, Kilner J, & Harrison L 2006, ‘A free energy principle for the brain’, Journal of Physiology-Paris, vol. 100, pp. 70-87. Available from:  https://www.fil.ion.ucl.ac.uk/~karl/A%20free%20energy%20principle%20for%20the%20brain. [1 May 2019].</li>
  <li>
    O’Connor, T & Wong, HY 2015, ‘Emergent Properties’, The Stanford Encyclopedia of Philosophy, 3 June. Available from: https://plato.stanford.edu/entries/properties-emergent/. [1 May 2019].
  </li>

  <li>Hawking, S 1999, Does God Play Dice, Available from: http://www.hawking.org.uk/does-god-play-dice.html. [1 May 2019].
  </li>
  <li>
Miles, R 2018, Why Would AI Want to do Bad Things? Instrumental Convergence, YouTube Video, 24 Mar. Available from: https://www.youtube.com/watch?v=wX78iKhInsc. [1 May 2019].
</li>

  <li>
    Closer To Truth Interview Series: What is Strong Emergence? Tim Maudlin n.d, television program, PBS. Available from: https://www.closertotruth.com/series/what-strong-emergence. [1 May 2019].
  </li>
  <li>
Closer To Truth Interview Series: What is Strong Emergence? Barry Loewer n.d, television program, PBS. Available from: https://www.closertotruth.com/series/what-strong-emergence. [1 May 2019].
  </li>
  <li>   
Kurgesagt - In a Nutshell 2017, Optimistic Nihilism, YouTube video, 26 Jul. Available from: https://www.youtube.com/watch?v=MBRqu0YOH14. [1 May 2019]. 
  </li>
</ul>













<!--
<p>Most intellectual disciplines have standard, unquestioned criteria for what counts as progress. Artificial intelligence is an exception. It has always borrowed criteria, approaches, and specific methods from at least six fields:</p>


<p>
1. Science<br />
2. Engineering<br />
3. Mathematics<br />
4. Philosophy<br />
5. Design<br />
6. Spectacle</p>
<p>This has always caused trouble. The diverse evaluation criteria are incommensurable. They suggest divergent directions for research. They produce sharp disagreements about what methods to apply, which results are important, and how well the field is progressing.</p>
<p>Can’t AI make up its mind about what it is trying to do? Can’t it just decide to be something respectable—science or engineering—and use a coherent set of evaluation criteria drawn from one of those disciplines?</p>
<p>That doesn’t seem to be possible. AI is unavoidably a <a href="https://en.wikipedia.org/wiki/Wolpertinger">wolpertinger</a>, stitched together from bits of other disciplines. It’s rarely possible to evaluate specific AI projects according to the criteria of a single one of them.</p>
<p>This post offers a framework for thinking about what makes the AI wolpertinger fly. The framework is, so to speak, parameterized: it accommodates differing perspectives on the relative value of criteria from the six disciplines, and their role in AI research. How they are best combined is a judgement call, differing according to the observer and the project observed. Nevertheless, one can make cogent arguments in favor of weighting particular criteria more or less heavily.<a class="see-footnote" id="footnoteref1_wozh6kz" title="This post echoes sections 9.1-9.2 of my PhD thesis, which proposed much the same framework. My main change of opinion since then is to put more weight on scientific truth criteria. I am now also skeptical of my claim there that AI is “about approaches,” as a legitimate autonomous source of value." href="#footnote1_wozh6kz">1</a></p>
<p>Choices about how to evaluate AI lead to choices about what problems to address, what approaches to take, and what methods to apply. I will advocate improving AI practice through greater use of scientific experimentation; pursuit particularly of philosophically interesting questions; better understanding of design practice; and greater care in creating spectacular demos. Follow-on posts will explain these points in more detail.</p>
<p>This framework is meant mainly for AI participants. For others, the pressing question may be “how long until superintelligent AI takes my job / makes us all rich without having to work / hunts down and kills all humans so it can <a href="https://wiki.lesswrong.com/wiki/Paperclip_maximizer">make more paperclips</a>.” I think the rational conclusion of a sophisticated, in-depth analysis, based on a detailed evaluation framework such as the one explored in this post, is: “Who knows?”</p>
<p>Some skepticism about near-term progress follows from considerations I’ll present here, though. AI has neglected scientific theory testing, and much of what the field thinks it knows may be false. And, demonstrations of apparent capabilities are often misleading.</p>
<p>The rest of this post has six sections explaining how progress criteria from the six disciplines work within AI; and then a concluding section recapitulating how I think they they should be weighted.</p>

<h2 id="science">Science</h2>
<p>Science’s progress criteria are:</p>
<div class="tight_list">
<ul>
<li>Newly-discovered truths</li>
<li>Broader explanations</li>
<li>An unusual sense of “interestingness,” related to, but not identical with, ordinary curiosity</li>
</ul>
<p>Let’s take them in order...</p>
</div>
<h3 id="defect">“The greatest defect”</h3>
<figure class="ctrimg_not_first"><img src="https://meaningness.com/images/mn/20000_squid_holding_sailor_409x599.jpg" width="409" height="599" alt="Giant squid attack!" title="Giant squid attack!" /></figure>
<p>The mainstream AI research program of the ’50s through the ’80s is now called “good old-fashioned AI” (GOFAI), since not many people pursue it anymore. GOFAI was exciting because it gave interesting, plausible explanations for how knowledge, reasoning, perception, and action work. For decades, we failed to put those theories to strenuous tests—and when we did, they turned out to false. Nearly everything we thought we knew was wrong. The GOFAI research program collapsed around 1990.</p>
<p>A. J. Ayer, a proponent of logical positivism in his youth, <a href="https://youtu.be/4cnRJGs08hE?t=6m28s">was asked</a> after it conclusively failed, “What do you now in retrospect think the main shortcomings of the movement were?” And he answered, “Well, I suppose the <em>greatest</em> defect is that nearly all of it was false!”<a class="see-footnote" id="footnoteref2_9dm2d58" title="It’s not coincidental that GOFAI largely recapitulated logical positivism. We were blithely ignorant of reinventing its pentagonal wheels, and of the reasons those don’t work. The Ayer interview video is entertaining and informative; thanks to Lucy Keer for pointing me to it." href="#footnote2_9dm2d58">2</a></p>
<p>GOFAI had several defects, but… the main thing is, nearly all of it was false. We should have realized this earlier, but we were distracted by fascinating philosophical and psychological questions, and by <em>wow, look at this cool thing we can make it do!</em></p>
<p>As far as current AI goes, the most important question is: what parts of it are true? It may have other virtues or defects, but until enough science is done to sort out which bits are just factually true, those are secondary.</p>
<p>Science aims to learn how the world works, by experiment when possible, or observation otherwise. In AI, we have the luxury of experiment. Still better: we have the luxury of <em>perfectly repeatable</em> experiments, under perfectly controlled conditions! Almost no other domain is as ideally suited to scientific investigation.</p>
<p>Yet it is uncommon for AI research to include either a hypothesis or an experiment. Papers commonly report work that <em>sort of sounds</em> like an experiment, but those often amount to:</p>
<blockquote>
<p>We applied an architecture of class X to a task in class Y and got Z% correct.</p>
</blockquote>
<p>There is no specific hypothesis here. Without a hypothesis, you are not doing a scientific experiment, you are just recording a factoid. Individual true facts (“the squid we caught today is Z% bigger than the last one!”) are not science without a testable general theory (“cold water causes <a href="https://en.wikipedia.org/wiki/Deep-sea_gigantism">abyssal gigantism</a> by way of extended lifespan”).</p>
<h3 id="explaining">Explaining AI</h3>
<p>Theories are much better if they are explanations, not just a formula for prediction. (Explanation is a criterion of scientific progress, although not an absolute requirement.) A good experiment should eliminate all but one possible explanation for the data, using controls.</p>
<p>Your algorithm got Z% correct: Why? What does that imply for performance on similar problems? AI papers often just speculate. Implicitly, the answer may be “we got Z% correct because architecture class X is awesomely powerful, and it will probably work for you, too!” The paper may state that “Z% is better than a previous paper that used an architecture of class W,” with the implication that X is better than W. But is it—in general?</p>
<p>Current machine learning research, by contrast with GOFAI, does not prioritize explanations. Sometimes it seems the field actively resists them. (I’ll suggest possible reasons below.) As far as scientific criteria go, without rigorous tests of explanatory hypotheses, you are left only with interestingness. Too often, interestingness (“Z% correct is awesome!”) is primary in public presentations of AI.</p>
<p>“This year, we’re getting Z% correct, whereas last year we could only get (Z-ε)%” does sound like progress. But is it meaningful? If the specific problem you are improving against is one people want solutions for, it may be <em>engineering</em> progress—discussed in the next section. It’s not scientific progress unless you understand where the improvement is coming from. Usually you can’t get that without extensive, rigorous experiments. You need to systematically test numerous variants of your program against numerous variants of the task, in order to isolate the factors that lead to success. You also need to test against entirely other architectures, and entirely other tasks.</p>
<p>This is a big job. Many researchers do <em>some</em> experiments of this sort. From individual projects, that may be the most we can reasonably expect, given limited resources. However, to adequately test hypotheses, the field as a whole needs to fill in the missing pieces—and often doesn’t. Its culture of competing against quantitative benchmarks encourages atheoretical tinkering, rather than science.</p>
<p>In many of the most-hyped recent AI “breakthroughs,” the control experiments that seem most obvious and important are missing. (I plan to discuss several of these in follow-up posts.)</p>
<h3>Is AI scientifically interesting?</h3>
<p>Because AI investigates <em>artificial</em> intelligence, its central questions are not necessarily scientifically interesting. They are interesting for biology only to the extent that AI systems deliberately model natural intelligence; or to the extent that you can argue that there is only one sort of computation that could perform a task, so biology and artificial intelligence necessarily coincide. This may be true of the early stages of visual processing, for example.</p>
<p>AI is mostly not about what nature <em>does</em> compute (science), nor about what we can compute today (engineering), nor about what could in principle be computed with unlimited resources (mathematics). It is about what <em>might</em> be computed by machines we might realistically build in the not-too-distant future. As this essay goes along, I will suggest that AI’s criterion of interestingness is therefore closer to that of philosophy of mind than to those of science, engineering, or mathematics.</p>
<h3 id="replicability">Learning from the replicability reform movement</h3>
<p>The “first principle” of science, Feynman said in his famous <a href="http://calteches.library.caltech.edu/51/2/CargoCult.htm">cargo cult address</a>, is that</p>
<blockquote>
<p>You must not fool yourself—and you are the easiest person to fool. So you have to be very careful about that. After you’ve not fooled yourself, it’s easy not to fool other scientists.</p>
</blockquote>
<p>The current <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> shows that many scientific fields have been fooling themselves on a massive scale. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/">Most published research findings are false</a>.</p>
<p>Social psychology is one field confronting this problem. Psychologists are engaged in impressive retrospective analysis, and in prospective <a href="https://en.wikipedia.org/wiki/Replication_crisis#Addressing_the_replication_crisis">reform efforts</a>. Meta-scientists in that field find that false conclusions are most likely when:</p>
<div class="tight_list">
<ul>
<li>Researchers pursue dramatic, surprising theories with implications for human nature and everyday life</li>
<li>Researchers and the media collaborate to spin exciting interpretive narratives for the public, generalizing well beyond specific findings</li>
<li>Researchers feel free to interpret their results after the fact</li>
<li>Researchers do not report null results (“failures”)</li>
<li>Researchers rarely repeat each other’s work to find problems</li>
<li>Researchers do not document their work in enough detail that others could check it</li>
<li>Experiments are done on an inadequate scale (in any of several dimensions)</li>
<li>Controls are missing or inadequate (in any of several ways)</li>
<li>Experiments are not systematically varied to find the limits of the theory</li>
</ul>
</div>
<p>These failures of scientific practice seem as common in AI research now as they were in social psychology a decade ago. From psychology’s experience, we should expect that many supposed AI results are scientifically false.</p>
<p>The problem—in both psychology and AI—is not bad scientists. It is that the communities have had <a href="/metablog/upgrade-your-cargo-cult#virtue">bad epistemic norms</a>: ones that do not reliably lead to new truths. Individual researchers do what they see other, successful researchers doing. We can’t expect them to do otherwise—not without a social reform movement.</p>
<p>The exciting news is that psychologists are taking these problems seriously. They are <a href="https://improvingpsych.org/mission/">putting in place</a> new epistemic norms that should help prevent such failures of scientific practice. These reforms should make discoveries of true, explanatory, interesting theories more common.</p>
<p>Can AI learn from psychology’s experience, to improve standards of practice?</p>
<p>I think it can, and should!</p>
<p>That said, AI is a wolpertinger. It’s not <em>just</em> science, and probably can’t just follow the replicability movement’s lead.</p>
<h2 id="engineering">Engineering</h2>
<p>Engineering applies well-characterized technical methods to well-characterized practical problems to yield well-characterized practical solutions.</p>
<p>Engineering’s progress criteria are quite different from science’s. If you discover new truths or explanations in the course of engineering, it’s incidental. And engineering isn’t supposed to be “interesting” in the scientific sense; instead, it is exciting when it yields practical value.</p>
<p>Engineering finds solutions within explicit constraints, and optimizes (or <a href="https://en.wikipedia.org/wiki/Satisficing">satisfices</a>) explicit objectives. Typically there are several, often with explicit numerical trade-offs between them. For instance: cost, safety, durability, reliability, ease of use, and ease of maintenance.</p>
<p>AI researchers often say they are doing engineering. This can sound defensive, when you point out that they aren’t doing science: “Yeah, well, I’m just doing engineering, making this widget work better.” It can also sound derisive, when you suggest that philosophical considerations are relevant: “I’m doing real work, so that airy-fairy stuff is irrelevant. As an engineer, I think metaphysics is b.s.”</p>
<p>Some AI work genuinely is engineering. Here’s the checklist:</p>
<ul>
<li>Does it apply well-characterized technical methods? Sometimes; but few AI methods are understood well.</li>
<li>Does it address well-characterized practical problems? Sometimes; but in research, AI most often gets applied to toy problems, not practical ones; and in industry, to poorly-characterized messes.</li>
<li>Does it yield well-characterized practical solutions? Sometimes you can say “our advertising click-through rate is up by 0.73%,” but if you don’t know quite why, that might reverse tomorrow.</li>
</ul>
<p>“Data science” is, in part, the application of AI (machine learning) methods to messy practical problems. Sometimes that works. I don’t know data science folks well, but my impression is that they find the inexplicability and unreliability of AI methods frustrating. Their perspective is more like that of engineers. And, I hear that they mostly find that well-characterized statistical methods work better in practice than machine learning.</p>
<p>Adjacent to engineering is the development of new technical methods. This is what most AI people most enjoy. It’s particularly satisfying when you <em>can</em> show that your new system architecture does Z% better than the competition. On the benchmark problem everyone is competing over… Does that reliably translate to real-world practice? Most AI researchers don’t want to take the time to find out. I will suggest below that this aspect of AI has more in common with design than engineering.</p>
<p>Engineering is great, when you can do it. Should AI be more like engineering? With much hard work, methods developed in AI research can sometimes be characterized well-enough that they get to be routinely used by engineers.</p>
<p>Then everyone stops calling it “AI.” This can be frustrating: every time we do something really great, it’s snatched away, and the field doesn’t get due credit. Unquestionably, AI research has spun off many of the most important advances in software technology. (Did you know that hash tables were long considered an advanced and incomprehensible AI technique?) Economically, AI research has been well worth the money spent on it.</p>
<p>But, the meaning of a word is in its use. “AI” is used to mean “complicated or hypothetical software that might be amazing, but we don’t understand why it works.” That simply isn’t engineering.</p>
<h2 id="mathematics">Mathematics</h2>
<p>Mathematics, like science, aims to discover interesting explanatory truths. What “interesting” and “explanatory” and “true” mean are quite different, and the methods—proof vs. experiment—are quite different.</p>
<p>Throughout its history, AI has shaded into mathematics, with results that contribute to both fields. This has often had powerful synergies.</p>
<p>That said, the evaluation criteria of mathematics—its senses of interesting, explanatory, and true—can be misleading in AI.</p>
<p>Proofs of algorithms’ asymptotic convergence are typical examples. Assuming a proof is technically correct, it is definitely true in the mathematical sense. It may exhibit structure that is mathematically explanatory: you have an “aha! so that’s why!” experience reading it. It is mathematically interesting if, for instance, it significantly generalizes an earlier result.</p>
<p>Most proofs of asymptotic convergence are <em>not</em> true, or explanatory, or interesting for AI, which has different criteria. AI is about physical realizability. That doesn’t have to mean “realizable using current technology,” but it does at least mean “realizable in principle.” A convergence result that shows an algorithm gets the right answer “in the limit” tells us nothing about physical realizability, even in principle. If quick arithmetic shows that the algorithm running on 10<sup>100</sup> GPUs will still be far from the answer after a trillion years, then the proof is not true, explanatory, or interesting—as AI. Conversely, unless you can demonstrate that an algorithm <em>will</em> converge reasonably quickly on realistic quantities of hardware, it’s not AI—however interesting it may be as math.</p>
<p>Mathematics is an invaluable tool. Using it well in AI requires subjecting it to alien evaluation criteria, from beyond math itself.</p>
<h2 id="philosophy">Philosophy</h2>
<figure class="ctrimg_not_first"><img src="https://meaningness.com/images/mn/Infinite_regress_of_homunculus_512x266.png" width="512" height="266" alt="Cartesian Theater" title="Cartesian Theater" /><figcaption class="ctrimgcaption" markdown="1">The infinite regress of the <a href="https://en.wikipedia.org/wiki/Homunculus_argument">Cartesian Theater</a><br />Image <a href="https://commons.wikimedia.org/wiki/File:Infinite_regress_of_homunculus.png">courtesy</a> Jennifer Garcia</figcaption></figure>
<p>Analytic philosophy—like science and mathematics—aims for interesting explanatory truths. It has, again, its own ideas of what count as “interesting,” “explanatory,” and “true.”</p>
<p>By and large, analytic philosophers start with “intuitions” they believe to be true, and then try to prove that they <em>are</em> true by way of arguments. I think the truth criterion “convincing arguments for intuitions” has been a bad influence on AI. It conflicts with science’s better criterion “neutral tests of hypotheses.” It has repeatedly led AI into making exaggerated claims based on inadequate evidence. I’ll suggest that analytic philosophy’s dysfunctional relationship with neuroscience has misled AI as well.</p>
<p>On the other hand, analytic philosophy of mind’s criterion for what counts as “interesting” largely coincides with, and formed, that of AI. From its founding, AI has been “applied philosophy” or “experimental philosophy” or “philosophy made material.” The hope is that philosophical intuitions could be <em>demonstrated</em> technically, instead of just argued for, which would be far more convincing. I share that hope.</p>
<p>Two fundamental intuitions most analytic philosophers of mind want to prove are:</p>
<div class="tight_list">
<ol>
<li>Materialism (versus mind/body dualism): mental stuff is really just physical stuff in your brain.</li>
<li>Cognitivism (versus behaviorism): you have beliefs, consider hypotheticals, make plans, and reason from premises to conclusions.</li>
</ol>
</div>
<p>These are apparently contradictory. “Hypotheticals” do not <em>appear</em> to be physical things. It is difficult to see how the belief “Gandalf was a wizard” could both be in your head and <em>about</em> Gandalf, as a physical fact. And so on.</p>
<p>This tension generated the problem space for GOFAI. The intuition of all cognitive scientists (including me! <a href="/metablog/ken-wilber-boomeritis-artificial-intelligence#AI">until 1986</a>) was that this conflict <em><a href="/wistful-certainty">must be</a></em> resolvable; and that its resolution could be proven, beyond all possibility of doubt, via technical implementation.</p>
<p>GOFAI papers largely described an implementation: the structure of a gizmo. (I’ll come back to this, in the section on design.) They usually also described an “experiment,” which rarely had scientific content: it was “we ran the program on three small inputs, and it produced the desired outputs.”</p>
<p>The exciting part of a GOFAI paper was the interpretive arguments. Starting from the structure of the gizmo, we made philosophical claims about the mind. The program, we said, was “learning from experience” or “reasoning about knowledge.” Its algorithm explained how those mental processes worked, at least roughly and for some cases, and probably for humans as well. These claims were often highly exaggerated, and mainly without scientific justification. In fact, the program built a labeled graph structure. We called that “knowledge”—but was it? Were these algorithms “learning” or “reasoning”? Ultimately, there is no fact-of-the-matter about this. But, it at least has to be argued for, and that part of the story was mostly missing. By systematically using the same words for human activities and simple algorithms, we deluded ourselves into confusing the map with the territory, and attributed mental activities to our programs just by fiat.</p>
<p>How did we go so wrong for so long with GOFAI? I think it was by inheriting a pattern of thinking from analytic philosophy: trying to prove metaphysical intuitions with narrative arguments. We <em>knew</em> we were right, and just wanted to <em>prove</em> it. And the way we went about proving it was more by argument than experiment.</p>
<p>Eventually, obstacles to the GOFAI agenda appeared to be matters of principle, not just matters of limited technical or scientific know-how, and it collapsed.</p>
<p>Some of us, at that point, went back and questioned AI’s fundamental philosophical assumption that cognitivism is the only alternative to behaviorism. We started a new line of research, pursuing a third alternative—interactionism—inspired by a different philosophical approach.</p>
<p>I believe AI’s best criterion of “interestingness” is philosophical, so that the proper business of AI research is to investigate philosophical questions. If so, a new philosophical approach was the right move! Evidence in favor of that were several technical breakthroughs. Perhaps we could and should have taken this line of work further.</p>
<p>After GOFAI’s collapse, philosophers gave up on AI. Most remained committed to cognitivism, so they transferred their hopes to neuroscience. Brains are obviously physical, mental, and cognitive, so they are definite proofs that materialism and cognitivism are right. (Right?) Thus the truth is established, and it goes without saying that minds are interesting, so all we need is an explanation. Philosophers encouraged neuroscientists to interpret their results in cognitivist terms. That has, I think, distorted neuroscience in much the same way it has distorted AI.</p>
<p>Thirty years later, we still <a href="https://mathbabe.org/2015/10/20/guest-post-dirty-rant-about-the-human-brain-project/">have no clue</a> what brains do or how.</p>
<blockquote>
<p>Neuro expectation: “Learn how we think and what makes us human!”<br /> Neuro reality: “Here are 30 different nuclei involved in eye movements!”<br /> —<a href="https://twitter.com/slatestarcodex/status/583071292015788032">Scott Alexander</a><br /> <br /> Actually: “Here are 30 different nuclei <em>correlated</em> with eye movement.”<br /> —<a href="https://twitter.com/michelteivel/status/583477174545580032">Michel Teivel</a></p>
</blockquote>
<p>In the absence of understanding, brains seem like magic. So, rather than trying to understand them scientifically, why not just <em>simulate</em> them, and gain the same powers? And maybe also it will be easier to run experiments on simulated brains than actual ones, and to gain understanding thereby.</p>
<p>From the beginning, AI has pursued this approach in parallel with GOFAI. Most of this research descends from McCullough and Pitts’s 1943 neuron model, which was biologically reasonable given the state of knowledge at the time. It also—<a href="http://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf">they pointed out</a>—neatly implemented <a href="/probability-and-logic#propositional">propositional logic</a>, which was still then a candidate for “<a href="https://en.wikipedia.org/wiki/The_Laws_of_Thought">The Laws of Thought</a>.” Subsequent research in the tradition has added technical features to the McCullough and Pitts model, motivated by computational considerations rather than biological ones. The most important is the error backpropagation algorithm, the central feature of contemporary “neural networks” and “deep learning.”</p>
<p>Meanwhile, neuroscience developed a much more complex and accurate understanding of biological neurons. These two lines of work have mainly diverged. Consequently, to the best of current scientific knowledge, AI “neural networks” work entirely differently from neural networks. Backpropagation itself does not seem biologically plausible (although, since we mostly don’t know how brains work, it can’t be ruled out).<a class="see-footnote" id="footnoteref3_j5j3acy" title="Biological considerations do continue to inspire some AI research. However, although much more detailed simulations of biological neurons are available, they are rarely used in AI. That’s probably in part because the best simulations are known not to incorporate much that’s known about neurons, and are known not to give quantitatively accurate results. It’s also because it’s not known how to combine multiple simulated neurons to perform computations that are interesting as AI." href="#footnote3_j5j3acy">3</a></p>
<p>Everyone in the field knows this, yet senior researchers still frequently talk as if “neural networks” work much like brains. I’ll suggest why later. But first, the <em>effect</em> of this rhetoric:</p>
<dl>
<dt>What makes your research program promising?</dt>
<dd>We are aiming for human-like intelligence, and our neural networks work like human brains.
</dd>
<dt>You mostly can’t explain why these systems work. Isn’t that a problem?</dt>
<dd>We don’t know how brains work, but they do, and the same is true for neural networks.
</dd>
<dt>Shouldn’t you be trying harder to find out how and when and why they work?</dt>
<dd>No, that’s probably impossible. Brains are holistic; you can’t understand them analytically.
</dd>
<dt>Some people say that they’ve analyzed specific “neural networks” and figured out how they do work. Turns out they do something boring, equivalent to kNN or even just regression.</dt>
<dd>But, you see, we’ve proven mathematically that neural networks have the flexibility to perform <em>any</em> computation. Like brains.
</dd>
<dt>So can my phone.</dt>
<dd>Yes, but phones aren’t like brains.
</dd>
</dl>
<p>This may be a comic exaggeration. But the sometimes-explicit, sometimes-tacit “works like brains” simultaneously explains why the research program <em><a href="/wistful-certainty">must</a></em> succeed overall, and waves away technical doubts about details.</p>
<p>This seems parallel to the pattern of error in GOFAI. We <em>knew</em> our “knowledge representations” couldn’t be anything like human knowledge, and chose to ignore the reasons why. Contemporary “neural network” researchers <em>know</em> their algorithms are nothing like neural networks, and choose to ignore the reasons why. GOFAI sometimes made wildly exaggerated claims about human reasoning; current machine learning researchers sometimes make wildly exaggerated claims about human intuition.</p>
<p>Why? Because researchers are trying to prove an <i>a priori</i> philosophical commitment with technical implementations, rather than asking scientific questions. The field measures progress in quantitative performance competitions, rather than in terms of scientific knowledge gained.</p>
<h2 id="design">Design</h2>
<p>I think AI researchers’ intuition is right that implementations—illustrative computer programs—are powerful sources of understanding. But how does that work? It’s tempting to analogize implementations to scientific experiments, but usually they aren’t. It’s tempting to think of them as engineering solutions, but they usually aren’t. I think “implementations” are best understood as <em>design solutions</em>—quite a different thing.</p>
<p>The actual practice of AI research is more like architectural design than like electrical engineering. Viewing AI through this lens helps explain its recurring destructive hype cycle pattern. I’ll explain how better design understanding may help evaluate AI progress more accurately, thereby smoothing the hype cycle.</p>
<p>The design view may also improve AI practice by eliminating a major source of technical difficulty and wasted effort.</p>
<h3 id="nature">The nature of design</h3>
<p>Design, like engineering, aims to produce useful artifacts. Unlike engineering, design addresses <a class="glossed" id="gloss0" title="Nebulosity is the insubstantiality, transience, boundarilessness, discontinuity, and ambiguity that (this book argues) are found in all phenomena. [Click for details.]" onclick="simple_glossary_onclick(event, 'nebulous', 'gloss0')">nebulous</a> (poorly characterized) problems; is not confined to explicit, rational methods; and develops snazzy—not optimal—solutions.</p>
<p>(Nebulosity is a matter of degree, so design and engineering shade into each other. Most designers do some engineering, and most engineers do some design. Temporarily polarizing the two helps explain how AI research is design-like.)</p>
<p>In engineering, you start with a well-specified problem statement. You begin by analyzing it to derive implications and constraints that guide your process. Only once you understand the problem throughly do you begin assembling a solution.</p>
<p>Design concentrates on synthesis, more than analysis. Since the problem statement is nebulous, it doesn’t provide helpful guiding implications; but neither does it strongly constrain final solutions. Design, from early in the process, constructs trial solutions from plausible pieces suggested by the concrete problem situation. Analysis is less important, and comes mostly late in the process, to evaluate how good your solution is.</p>
<p>Since design problems are nebulous, there is no such thing as an optimal solution. The evaluation criterion might be called “snazziness” instead. A good design is one people <em>like</em>. It should make you go “whoa, cool!” An outstanding design <em>amazes</em>. Design success means not that you solved a specific problem as given, but that you produced something both nifty <em>and</em> useful in a general vicinity. (The products of design, unlike those of art, have to work as well as wow.)</p>
<h3 id="practice">Design in practice</h3>
<figure class="ctrimg_not_first"><img src="https://meaningness.com/images/mn/architectural-model-nichols-house-560x360.jpg" width="560" height="360" alt="Architectural model" title="Architectural model" /><figcaption class="ctrimgcaption" markdown="1">Image <a href="https://collections.museumvictoria.com.au/items/255676 ">courtesy</a> Museums Victoria</figcaption></figure>
<p>Systematic, explicit, rational methods are secondary in design. Those mostly don’t apply to nebulous problems with nebulous solution criteria. Expert designers say they rely instead on “creativity” and “intuition.” That isn’t helpful; it just means “we don’t know how we do it.” Indeed, design competence is largely tacit, inarticulable, and “know-how” more than “knowing-that.” For that reason, it has to be learned through apprenticeship and experience, rather than in classrooms or through reading.</p>
<p>Nevertheless, empirical studies of design practice give some insight into how it works.<a class="see-footnote" id="footnoteref4_qapn6dd" title="See, for example, Donald Schön’s The Reflective Practitioner: How professionals think in action, and Nigel Cross’ Designerly Ways of Knowing." href="#footnote4_qapn6dd">4</a></p>
<p>First, a designer maintains contact with the messy concrete specifics of the problem throughout the process. An engineer, by contrast, operates primarily in a formal domain, abstracted from the mess.</p>
<p>Metaphorically, possible design approaches are <em>suggested</em> by the mess. From these suggestions, the designer builds a series of quick-and-dirty prototype models, and tries them out to see how they work. Architects build models from cardboard; AI researchers build them from code. These prototypes are not engineering models, subjected to serious real-world testing. They’re just “sketches” to give a sense of how something <em>might</em> work.</p>
<p><a href="/further-reading#Schön">Donald Schön</a> describes this cycle as a “reflective conversation with the materials.” Having the model provides concreteness, again, that guides the next step. You can “sort of see” how it will or won’t work. You build up an understanding of the problem space by trying out diverse possibilities, and then by iterative improvement of a promising candidate. The understanding gained is explanatory, but as with design knowledge in general, it is partly tacit, inarticulable know-how; a felt sense of how things work.</p>
<p>The design process repeatedly transforms the problem itself, which remains fluid throughout. What you think you are trying to accomplish changes repeatedly. The solution defines the problem as much as vice versa. You want to create <em>something snazzy</em> in the general area; and what “snazzy” <em>means</em> emerges only as a concrete property of the final product.</p>
<p>For engineers, this may seem highly unsatisfactory. Wouldn’t it be better to nail down exactly what the problem is, figure out what would make for a quantitatively good solution, and apply rational methods to get from here to there, instead of “having a conversation with a mess?”</p>
<p>If you can do that—it’s often the best approach. That’s why engineering is valuable. But many real-world situations <a href="https://en.wikipedia.org/wiki/Wicked_problem">just don’t</a> resolve neatly into well-defined problems.</p>
<h3 id="AI">AI research as design practice</h3>
<figure class="ctrimg_not_first"><img src="https://meaningness.com/images/mn/chinese-latin-dictionary-305x255.gif" width="305" height="255" alt="Chinese-Latin grammar, Fourmont, 1742" title="Chinese-Latin grammar, Fourmont, 1742" /></figure>
<p>As noted above in the section on AI as engineering, AI typically applies ill-characterized methods to nebulous problems with nebulous solution criteria. (Using neural networks to translate Mandarin Chinese to English, for example.) In at least this way, it resembles design practice.</p>
<p>If you can nail down the problem, eliminate nebulosity, and demonstrate correctness, you are doing mainstream computer science, not AI. Which is great! But not always possible. No one can say what the problem of translation <em>is</em>, and there is no such thing as an optimal translation. But, your aim as an AI researcher is to do it <em>well enough</em> to impress people. That would definitely be snazzy!</p>
<p>So, you start hacking. You build a series of quick-and-dirty prototypes, and try them out on some Mandarin texts to see how they work. The different patterns of good and bad translations the programs produce suggests each next implementation. It may be difficult to say exactly what those patterns are, but you gradually build up insight into what works and why. And as you proceed, your understanding of what translation even <em>means</em> changes. This is your “reflective conversation with the concrete materials”—which include both natural language texts and program structure.</p>
<p>So in AI we build implementations to gain an understanding, which we may not be able to fully articulate. The implementation <em>embodies</em> the understanding, and can <em>communicate</em> the understanding. To develop expertise in AI, you can’t just read papers; you have to read other people’s code. And you can’t just read it, you have to <em>re</em>-implement it. Part of your understanding is gained only through the practice of coding itself. You don’t really know what a neural network is until you’ve written a backpropagation engine from scratch yourself, and run it against some classic small data sets, and puzzled over its outputs.</p>
<h3 id="skills">A skills mismatch</h3>
<p>AI researchers are mostly educated in fields that take formal problems as inputs: engineering, mathematics, or theoretical physics. Yet the problems we tackle are mostly ones in which a design approach, maintaining a continuous, open-ended relationship with nebulosity, may be more appropriate.</p>
<p>You can’t learn how to relate to a mess in a classroom, by reading, or from Coursera. It is possible to learn from hard experience. It is better learned by apprenticeship. I gather that industry currently understands that there is <em>something</em> critical that PhDs from the best academic AI labs learn by apprenticeship, which can’t be learned any other way. I suspect it’s this.</p>
<p>Having been taught mainly skills for solving formal problems, AI folks tend to jump away from nebulosity as quickly as possible. Rather than slogging through the swampy real world, allowing informative patterns to gradually emerge, it’s more comfortable to escape into analyzing the nearest available abstraction.</p>
<p>So, premature problem formalization is a characteristic failure mode in AI. A nebulous real-world phenomenon (learning, for instance) gets replaced by some bit of mathematics (function approximation, for instance). The real-world word (“learning”) gets applied to both, interchangeably, so that researchers don’t even notice the difference. Then you can have all kinds of fun inventing and improving snazzy gizmos that address this precise but inaccurate problem statement. That may lead to valuable technical progress. Function approximation is a thing, and better methods have extensive engineering applications.</p>
<p>On the other hand, function approximation is not actually learning. Premature formalization means that solutions to the abstract mathematical problem may not be solutions to the concrete real-world problem, and vice versa.</p>
<p>This leads to two characteristic patterns of trouble. First, the abstract problem may be harder than the concrete one, because it elides key helpful features. In design theory terms, you are failing to listen to suggestions murmured by the mess. For example, the GOFAI plan-based formalization of practical action made the problem much more difficult than it needed to be, because it threw away on-going perceptual access to relevant information. Phil Agre and I <a href="https://dspace.mit.edu/handle/1721.1/6487">wrote programs</a> that went far beyond what the planning approach was capable of, by transforming the statement of the problem.</p>
<p>Alternatively, the abstract problem may be easier than the concrete one. This can lead to overconfidence and hype. In evaluating AI, one needs to be skeptical of researchers’ claim that they are making rapid progress on problem “X.” Are they actually working on the real-world task X? Or are they solving a formal problem they have abstracted from X, and applying the same name to it? For example, are they making progress on learning to translate Mandarin to English (a real-world problem), using neural networks? Or are they making progress on a formal problem which might better be described as “storing n-gram pairs in a lookup table,” using gradient descent on a continuous function? (A sadly expensive and unreliable way of implementing a lookup table.)</p>
<p>When the difference between the two manifests as poor performance in the real world, this leads to disillusionment and loss of funding.</p>
<h3 id="antidotes">Antidotes</h3>
<figure class="ctrimg_not_first"><img src="https://meaningness.com/images/mn/Juicy_Salif_319x480.jpg" width="319" height="480" alt="Lemon squeezer" title="Lemon squeezer" /><figcaption class="ctrimgcaption" markdown="1">“Juicy Salif” lemon squeezer designed by Phillipe Starck<br />Image <a href="https://en.wikipedia.org/wiki/Lemon_squeezer#/media/File:Juicy_Salif_-_78365.jpg">courtesy</a> Niklas Morberg</figcaption></figure>
<p>I will suggest two antidotes. The first is the design practice of maintaining continuous contact with the concrete, nebulous real-world problem. Retreating into abstract problem-solving is tidier but usually doesn’t work well. IOU: my planned next post makes more detailed recommendations for better AI practice through insights from design practice.</p>
<p>Second: wolpertinger to the rescue! AI is not <em>just</em> design; it also draws from engineering, math, science, and philosophy.</p>
<ul>
<li>While squeezing lemon over your dish of calamari, you are inspired to create a spectacular new design for a snazzy squid-shaped lemon squeezer.<a class="see-footnote" id="footnoteref5_qbfphpj" title="This actually happened to Phillipe Starck. His process in this invention is analyzed in detail in Nigel Cross’ Design Thinking: Understanding How Designers Think and Work, working from the napkin on which Starck sketched successive design attempts. The final product is  considered an icon of industrial design and has been displayed in New York's Museum of Modern Art. There is no accounting for taste." href="#footnote5_qbfphpj">5</a> Or, you <em>hope</em> it’s snazzy. Now it’s time for <strong>engineering</strong>: can you make it affordable, safe, durable, reliable, easy to use, and easy to clean? By analogy: you’ve coded a snazzy new function approximation method. You want everyone to use it. That means you have to iron out all the niggling bugs and performance problems, and characterize convergence and scaling in diverse realistic scenarios. This may require difficult <strong>math</strong> as well as engineering tests.</li>
<li>You’ve studied the 30 different nuclei correlated with eye movements, and you developed a neural network model for them. You connected it up with a robot camera motion controller. Very cool! Now it’s time for <strong>science</strong>: how well can you predict human or animal eye movements? What other models might account for eye movement? How can you test which model is correct? What evidence would discriminate?</li>
<li>You have a new theory for the mental representation of knowledge. And you coded it up! Now it’s time for <strong>philosophy</strong>: what do you mean, “representation” and “knowledge”? These are unavoidable philosophical questions that need substantive answers. You can’t fall back on “Hey, I’m just doing engineering, man.”</li>
</ul>
<h2 id="spectacle">Spectacle</h2>
<figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/gIjozvi-tDk?rel=0" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>
<p>Spectacle is an essential component of any professional practice, including science, engineering, mathematics, philosophy, and design.</p>
<p>It is natural and legitimate to want to amaze people. You are excited about your research program, and you want to share that. Your research is probably also driven by particular beliefs, and it’s natural to want to persuade people of them. A spectacular demonstration can change beliefs, and whole ways of thinking, in minutes—far faster than any technical exposition or logical argument.</p>
<p>Plus, there’s always competition for resources—money, attention, smart people. It’s legitimate to make the best honest case for your work, and that of others in your subfield who share your beliefs. A spectacular demonstration is more effective than any whitepaper or funding proposal.</p>
<p>Success criteria for spectacle include drama, narrative, excitement, and (most importantly) incentive to action. The entertainment industry is the natural home of spectacle. In that industry (including its subsectors such as politics, the news, and professional wrestling) truth is not a consideration.</p>
<p>In disciplines concerned with truth—which should include AI—one must design demonstrations with a special type of conscientiousness. Because spectacle is so powerful, it’s morally imperative to go beyond mere factual honesty, lest you fool both yourself and others. A spectacle must take great care not to implicitly imply greater certainty, understanding, or interestingness than your research justifies.</p>
<p>In AI spectacles, the great danger is giving the impression that a program can do more than it does in reality; or that what it does is more interesting than it really is; or that the explanation of how it works is more exciting than reality. If an audience learns a true fact, that the program does X in a particular, dramatic case, it’s natural to assume it can do X in most seemingly-similar cases. But that may not be true.</p>
<p>Imagine watching a TV advertisement for “a fully automatic dishwasher!” in the 1950s, before you knew what one was. It shows Mom grimacing at a disorderly pile of dirty dishes in the sink. <a href="https://en.wikipedia.org/wiki/Wipe_(transition)">Clock-wipe video transition</a> to: Mom smiling at neatly stacked, glistening dishes on the counter!</p>
<p>You might reasonably assume that a “dishwasher” was a robot with two arms that stood by the sink and washed dishes by hand. It’s spectacular what technology can do now in the 1950s! Why, if a robot can wash the dishes, it can surely also vacuum the floor, change the baby, and make the bed. That would be a reasonable conclusion—if a dishwasher worked that way.</p>
<p>A dishwasher has superhuman performance; mine gets glasses shinier than I can, for far less effort. The advertisement is not lying about that.</p>
<p>But the clock-wipe concealed essential facts about <em>how</em> the dishwasher worked. It’s just a box that sprays hot water inside, not a robot. Its performance does not extend to household tasks of apparently similar difficulty, because they are not <em>relevantly</em> similar, in the way that is obvious when you know how it works.</p>
<p>A dishwasher also does not do the part of the task that would be most difficult for a robot: picking up irregularly-placed dishes smeared in greasy sauce. Fortunately, that is easy for people: loading the dishwasher is quick for us, relative to washing. Also not obvious from the advertisement: the dishwasher doesn’t quite do the whole job: you have to wash large pots and delicate glasses by hand.</p>
<p>Spectacular AI demos are often misleading in analogous ways. They rarely, if ever, convey an accurate understanding of how the program works. To be fair, it’s nearly impossible to do that in a demo, and it’s not the function of demos. But if they tacitly convey a <em>wrong</em> understanding, rather than just prompting curiosity, the audience gains a mistaken expectation for what else the program can do. Such misunderstandings are particularly likely if the demo glosses over parts of the task that the audience would reasonably assume the program does, but which are omitted because—like picking up greasy plates—they are particularly difficult for a computer. In current work, this might include feature engineering, for instance.</p>
<p>There is, almost always, much less to spectacular AI “successes” than meets the eye. But this deception, even though it is usually unintended, takes in researchers as well as outsiders. (“You are the easiest person to fool.”) This dynamic contributes to AI’s perennial hype cycle—exaggerated expectations that can’t be met, followed by disillusionment and funding “<a href="https://en.wikipedia.org/wiki/AI_winter">winters</a>.”</p>
<figure class="ctrimg"><div class="youtube_outer_wrapper"><div class="youtube_inner_wrapper"><iframe class="youtube" src="https://www.youtube-nocookie.com/embed/QAJz4YKUwqw?rel=0" width="560" height="315" frameborder="0" allowfullscreen=""></iframe></div></div></figure>
<p>A dialog produced in 1970 by Terry Winograd’s SHRDLU “natural language understanding” system was perhaps the most spectacular AI demo of all time. (You can read the whole dialog <a href="http://hci.stanford.edu/winograd/shrdlu/">on his web site</a>, download <a href="http://hci.stanford.edu/winograd/shrdlu/code.tar">the code</a>, or watch the demo <a href="https://www.youtube.com/watch?v=QAJz4YKUwqw">on YouTube</a> above.)</p>
<p>The sophistication of the program’s apparent language understanding is extraordinary. It bests current systems, such as Siri, Alexa, and Google Assistant, on which (it is said) billions of dollars of AI research have been spent <em>half a century later</em>. SHRDLU provided a warm glow of confidence that AI was achievable, and that GOFAI was progressing, for the next fifteen years.</p>
<p>There was nothing dishonest in Winograd’s work; no deliberate deception. However, by 1986, he came to believe he had fooled himself, and the field as a whole. In <cite><a href="/further-reading#Winograd">Understanding Computers and Cognition</a></cite>, he argued that SHRDLU’s understanding was merely apparent. Winograd gave strong reasons to believe that computers can’t understand natural language at all, even in principle. At least not using GOFAI methods: it’s the <em>how</em> that matters.</p>
<p>Analogously, I believe there is significantly less to current spectacular demos of “deep learning” than meets the eye. This is not mainly general cynicism about spectacles, nor skepticism about AI demos in general, nor dislike of deep learning in particular. (Although the deep learning field’s relative lack of interest in explanation does make it easier for researchers to fool themselves.) Primarily, it’s based on my guesses about specifically how these systems accomplish the tasks they are shown performing in the demos; and from that, how likely they are to accomplish tasks that may appear similar but aren’t. (I hope to analyze some examples in a follow-on post.)</p>
<p>Dishwashers weren’t on the path to general-purpose household robots. I don’t think current machine learning research will be either. Still, the technologies used in dishwashers have led to a continuing stream of labor-saving appliances. (I love my <a href="http://www.amazon.com/dp/B00FLYWNYQ/?tag=meaningness-20">Instant Pot</a>!) The technologies used in current AI demos may lead to a continuing stream of mental-effort-saving software.</p>
<h2 id="wolpertinger">Soaring Wolpertinger: Better AI through meta-rationality</h2>
<p><dfn><a class="glossed" id="gloss1" title="Meta-rationality means thinking about and acting on rational systems from the outside, in order to use them more effectively. It evaluates, selects, combines, modifies, discovers, and creates rational methods. Meta-rationalism is an understanding of how and when and why rational systems work. It avoids taking them as fixed and certain, and thereby avoids both cognitive nihilism and rationalist eternalism. [Click for details.]" onclick="simple_glossary_onclick(event, 'Meta-rationality', 'gloss1')">Meta-rationality</a></dfn> means figuring out how to use technical rationality in specific situations. (I am writing <a href="/eggplant">a book</a> about this.)</p>
<p>Artificial intelligence requires meta-rationality for two reasons. First, the problems it addresses are inherently nebulous. Rational methods, unaided, are not usually adequate in nebulous messes; without a specific problem statement, they can’t even get started.</p>
<p>Secondly, AI is a wolpertinger: not a coherent, unified technical discipline, but a peculiar hybrid of fields with diverse ways of seeing, diverse criteria for progress, and diverse rational and non-rational methods. Characteristically, meta-rationality evaluates, selects, combines, modifies, discovers, creates, and monitors multiple frameworks.</p>
<p>So, necessarily, does AI. It unavoidably combines disparate perspectives and ways of thinking. You need meta-rational skill to figure out which of these frameworks to apply, and how.</p>
<p>AI also unavoidably involves multiple, incommensurable progress criteria. I began this post by asking “how should we evaluate progress in AI?” The answer was “lots of ways!”</p>
<p>And so we should try to do better along <em>lots</em> of axes. In this post, I have particularly advocated increased consideration of criteria and methods:</p>
<div class="tight_list">
<ul>
<li>Of truth, from science</li>
<li>Of understanding, from design</li>
<li>Of interestingness, from philosophy</li>
</ul>
</div>
<p>We can, and should, disagree about how heavily to weight these and other considerations. A healthy intellectual field engages in continuous, contentious, collaborative reflection upon its own structure, norms, assumptions, and commitments. This was the point of my “<a href="/metablog/upgrade-your-cargo-cult">Upgrade your cargo cult for the win</a>,” especially in <a href="/metablog/upgrade-your-cargo-cult#upgrade">its conclusion</a>.</p>
<p>It’s also the central theme of my sometime-collaborator’s Philip Agre’s <a href="http://www.amazon.com/dp/0521386039/?tag=meaningness-20"><cite>Computation and Human Experience</cite></a>, which discusses in greater depth most of the ideas I’ve presented in this essay.<a class="see-footnote" id="footnoteref6_9s9ushm" title="It was re-reading Phil’s book, as background for working on In the Cells of the Eggplant, that inspired me to write this post." href="#footnote6_9s9ushm">6</a></p>
<h2>Postscript</h2>
<p>A week after I posted this, Zachary Lipton and Jacob Steinhardt posted “<a href="http://approximatelycorrect.com/2018/07/10/troubling-trends-in-machine-learning-scholarship/">Troubling Trends in Machine Learning Scholarship</a>,” which makes quite similar arguments, but with many detailed examples from recent work. I recommend it as an excellent, up-to-the-minute analysis by experts in the current state of the field.</p>

<ul class="footnotes"><li class="footnote" id="footnote1_wozh6kz"><a class="footnote-label" href="#footnoteref1_wozh6kz">1.</a> This post echoes sections 9.1-9.2 of <a href="http://www.amazon.com/dp/0262031817/?tag=meaningness-20">my PhD thesis</a>, which proposed much the same framework. My main change of opinion since then is to put more weight on scientific truth criteria. I am now also skeptical of my claim there that AI is “about approaches,” as a legitimate autonomous source of value.</li>
<li class="footnote" id="footnote2_9dm2d58"><a class="footnote-label" href="#footnoteref2_9dm2d58">2.</a> It’s not coincidental that GOFAI largely recapitulated logical positivism. We were blithely ignorant of reinventing its pentagonal wheels, and of the reasons those don’t work. The Ayer interview video is entertaining and informative; thanks to Lucy Keer for pointing me to it.</li>
<li class="footnote" id="footnote3_j5j3acy"><a class="footnote-label" href="#footnoteref3_j5j3acy">3.</a> Biological considerations do continue to inspire some AI research. However, although much more <a href="https://en.wikipedia.org/wiki/Biological_neuron_model">detailed simulations of biological neurons</a> are available, they are rarely used in AI. That’s probably in part because the best simulations are known not to incorporate much that’s known about neurons, and are known not to give quantitatively accurate results. It’s also because it’s not known how to combine multiple simulated neurons to perform computations that are interesting as AI.</li>
<li class="footnote" id="footnote4_qapn6dd"><a class="footnote-label" href="#footnoteref4_qapn6dd">4.</a> See, for example, Donald Schön’s <cite><a href="/further-reading#Schön">The Reflective Practitioner: How professionals think in action</a></cite>, and Nigel Cross’ <a href="http://www.amazon.com/dp/B000SNUQ7Q/?tag=meaningness-20"><cite>Designerly Ways of Knowing</cite></a>.</li>
<li class="footnote" id="footnote5_qbfphpj"><a class="footnote-label" href="#footnoteref5_qbfphpj">5.</a> This actually happened to Phillipe Starck. His process in this invention is analyzed in detail in Nigel Cross’ <a href="http://www.amazon.com/dp/1847886361/?tag=meaningness-20"><cite>Design Thinking: Understanding How Designers Think and Work</cite></a>, working from the napkin on which Starck sketched successive design attempts. The final product is  considered an icon of industrial design and has been displayed in New York's Museum of Modern Art. There is no accounting for taste.</li>
<li class="footnote" id="footnote6_9s9ushm"><a class="footnote-label" href="#footnoteref6_9s9ushm">6.</a> It was re-reading Phil’s book, as background for working on <cite><a href="/eggplant">In the Cells of the Eggplant</a></cite>, that inspired me to write this post.</li>
</ul>
</div></div></div>  </div>

      <nav class="clearfix"><ul class="links inline"><li><span>
<iframe src="//www.facebook.com/plugins/like.php?href=https%3A%2F%2Fmeaningness.com%2Fmetablog%2Fartificial-intelligence-progress&amp;layout=button_count&amp;show_faces=false&amp;width=80&amp;font=arial&amp;height=20&amp;action=like&amp;colorscheme=light&amp;send=false&amp;share=false" scrolling="no" frameborder="0" style="border: none; overflow: hidden; width: 80px; height: 20px; vertical-align:bottom;" allowTransparency="true"></iframe>
</span></li><li><a href="http://twitter.com/?status=How%20should%20we%20evaluate%20progress%20in%20AI%3F%20https%3A//meaningness.com/metablog/artificial-intelligence-progress" class="tweet" rel="nofollow" target="_blank"><img class="image-style-none" src="https://meaningness.com/sites/all/modules/tweet/twitter.png" alt="&amp;nbsp; Tweet" title="&amp;nbsp; Tweet" /> &nbsp; Tweet</a></li><li><a href="/metablog/artificial-intelligence-progress/comments" class="talk_page_link">22 Comments</a></li></ul></nav>
  
  
  </article>

</div>              </div>
            
            
            
          </section>

          <div class="region region-content-aside"><div class="region-inner clearfix"><section id="block-meaningness-navigation-0" class="block block-meaningness-navigation odd first block-count-2 block-region-content-aside block-0" ><div class="block-inner clearfix">  
      <h2 class="block-title">Navigation</h2>
    
  <div class="block-content content"><div class="meaningness_navigation_content"><p>You are reading a metablog post, dated June 30, 2018.</p><p>This was the most recent metablog post.</p><p><span class="navigation_icon">&#x261C</span> The previous metablog post was <a href="/metablog/circumscription-farce">Circumscription: a logical farce</a>.</p><p>This page&rsquo;s topic is <a href="/taxonomy/term/30">Rationalism</a>.</p><p><span class="navigation_explanation">General explanation</span>: <cite>Meaningness</cite> is a hypertext book (in progress), plus a &ldquo;<a href="/metablog">metablog</a>&rdquo; that comments on it. The book begins with <a href="/an-appetizer-purpose">an appetizer</a>. Alternatively, you might like to look at its <a href="/#contents">table of contents</a>, or some other <a href="/starting-points">starting points</a>. Classification of pages by <a href="/topics">topics</a> supplements the book and metablog structures. Terms with dotted underlining (example: <a class="glossed" id="gloss0" title="“Meaningness” is the quality of being meaningful and/or meaningless. It has various dimensions, such as value, purpose, and significance. This book suggests that meaningness is always nebulous&mdash;ambiguous and fluid&mdash;but also always patterned. Confusion about meaningness results from denying nebulosity or fixating pattern. [Click for details.]" onclick="simple_glossary_onclick(event, 'meaningness', 'gloss0')">meaningness</a>) show a definition if you click on them. Pages marked with ⚒ are still under construction. Copyright &copy;2010–2019 David Chapman.</p></div></div>
</div></section><div id="block-search-form" class="block block-search no-title even last block-count-3 block-region-content-aside block-form"  role="search"><div class="block-inner clearfix">  
    
  <div class="block-content content"><form action="/metablog/artificial-intelligence-progress" method="post" id="search-block-form" accept-charset="UTF-8"><div><div class="container-inline">
      <h2 class="element-invisible">Search form</h2>
    <div class="form-item form-type-textfield form-item-search-block-form">
  <label class="element-invisible" for="edit-search-block-form--2">Search </label>
 <input title="Enter the terms you wish to search for." type="search" id="edit-search-block-form--2" name="search_block_form" value="" size="15" maxlength="128" class="form-text" />
</div>
<div class="form-actions form-wrapper" id="edit-actions--2"><input type="submit" id="edit-submit--2" name="op" value="Search" class="form-submit" /></div><input type="hidden" name="form_build_id" value="form-denv8zmJNxQGSPug8QJimI6RRwzyS4TYMuPeDvWn0KE" />
<input type="hidden" name="form_id" value="search_block_form" />
</div>
</div></form></div>
</div></div></div></div>
        </div></div>

                
      </div></div>
    </div></div>

          <div id="tertiary-content-wrapper">
        <div class="container clearfix">
          <div class="region region-tertiary-content"><div class="region-inner clearfix"><section id="block-block-1" class="block block-block odd first block-count-4 block-region-tertiary-content block-1" ><div class="block-inner clearfix">  
      <h2 class="block-title">Get updates</h2>
    <h3 class="block_subtitle"><em>Notification</em> of <em>new</em> pages.</h3>
  <div class="block-content content"><form id="mc-signup-form" action="//meaningness.us4.list-manage.com/subscribe/post?u=bf2cfe69c9a120e828f837dd8&amp;id=e8a013f1cd" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
  <input type="email" placeholder="Enter your email address" value="" name="EMAIL" id="mce-EMAIL" />
  
  <input type="text" name="b_bf2cfe69c9a120e828f837dd8_e8a013f1cd" tabindex="-1" value="" style="position: absolute; left: -5000px;" aria-hidden="true" />
  <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" />
</form>
<p><a href="/email-subscriptions"><img src="/images/envelope.png" alt="Subscribe by email" title="Subscribe by email" width="16" height="16" /> More email options &amp; info</a></p>
<p><a href="/feeds"><img src="/misc/feed.png" alt="Syndicate content" title="Syndicate" width="16" height="16" /> RSS feed</a></p>
<p><a href="http://twitter.com/Meaningness"><img src="/images/twitter_icon.png" alt="Follow Meaningness on Twitter" /> Follow me on Twitter</a></p>

</div>
</div></section><section id="block-views-recent-pages-block-1" class="block block-views even block-count-5 block-region-tertiary-content block-recent-pages-block-1" ><div class="block-inner clearfix">  
      <h2 class="block-title">Recent pages</h2>
    <h3 class="block_subtitle"><em>Hot</em> off the <em>Interwebs</em>.</h3>
  <div class="block-content content"><div class="view view-recent-pages view-id-recent_pages view-display-id-block_1 view-dom-id-a7044b12bfe89390e1bb4712571577ca">
        
  
  
      <div class="view-content">
      <div class="item-list">    <ul>          <li class="views-row views-row-1">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/cofounders-in-relationship">The Cofounders</a></span>  </div></li>
          <li class="views-row views-row-2">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/eggplant/pebbles">The parable of the pebbles</a></span>  </div></li>
          <li class="views-row views-row-3">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/eggplant/terms">Introducing key terms</a></span>  </div></li>
          <li class="views-row views-row-4">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/metablog/artificial-intelligence-progress" class="active">How should we evaluate progress in AI?</a></span>  </div></li>
          <li class="views-row views-row-5">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/metablog/circumscription-farce">Circumscription: a logical farce</a></span>  </div></li>
          <li class="views-row views-row-6">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/podcast-appearances">Podcast appearances</a></span>  </div></li>
          <li class="views-row views-row-7">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/metablog/post-apocalyptic-health-care">Post-apocalyptic life in American health care</a></span>  </div></li>
          <li class="views-row views-row-8">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/further-reading">Appendix: Further reading</a></span>  </div></li>
          <li class="views-row views-row-9">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/metablog/meta-rationality-questions">Your questions about meta-rationality?</a></span>  </div></li>
          <li class="views-row views-row-10">  
  <div class="views-field views-field-title">        <span class="field-content"><a href="/eggplant/function-structure">The function and structure of the eggplant</a></span>  </div></li>
      </ul></div>    </div>
  
  
  
      
<div class="more-link">
  <a href="/recent-pages">
    More pages...  </a>
</div>
  
  
  
</div></div>
</div></section><section id="block-views-recent-comments-block-1" class="block block-views odd block-count-6 block-region-tertiary-content block-recent-comments-block-1" ><div class="block-inner clearfix">  
      <h2 class="block-title">Discussion</h2>
    <h3 class="block_subtitle">Read <em>comments</em> and <em>join</em> in.</h3>
  <div class="block-content content"><div class="view view-recent-comments view-id-recent_comments view-display-id-block_1 view-dom-id-bac7e13d3c1702c81fdfe803cda8e296">
        
  
  
      <div class="view-content">
      <div class="item-list">    <ul>          <li class="views-row views-row-1 views-row-odd views-row-first">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/cofounders-in-relationship/comments/#comment-2229">Also Jennifer Garvey Berger</a></span>  </div></li>
          <li class="views-row views-row-2 views-row-even">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/metablog/sad-light-led-lux/comments/#comment-2228">Nomadism = SAD solution</a></span>  </div></li>
          <li class="views-row views-row-3 views-row-odd">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/big-three-stance-combinations/comments/#comment-2227">I haven’t read much by him,</a></span>  </div></li>
          <li class="views-row views-row-4 views-row-even">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/cofounders-in-relationship/comments/#comment-2226">Beta-testing the workbook</a></span>  </div></li>
          <li class="views-row views-row-5 views-row-odd">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/metablog/sad-light-led-lux/comments/#comment-2225">SAD light update</a></span>  </div></li>
          <li class="views-row views-row-6 views-row-even">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/metablog/sad-light-led-lux/comments/#comment-2224">Hack for SAD Light therapy</a></span>  </div></li>
          <li class="views-row views-row-7 views-row-odd">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/monism-dualism-countercultures/comments/#comment-2223">Current political trends</a></span>  </div></li>
          <li class="views-row views-row-8 views-row-even">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/metablog/sad-light-led-lux/comments/#comment-2222">updates?</a></span>  </div></li>
          <li class="views-row views-row-9 views-row-odd">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/cofounders-in-relationship/comments/#comment-2221">Great post!</a></span>  </div></li>
          <li class="views-row views-row-10 views-row-even views-row-last">  
  <div class="views-field views-field-subject">        <span class="field-content"><a href="/geeks-mops-sociopaths/comments/#comment-2220">Or maybe there’s a small</a></span>  </div></li>
      </ul></div>    </div>
  
  
  
      
<div class="more-link">
  <a href="/recent-comments">
    More comments . . .  </a>
</div>
  
  
  
</div></div>
</div></section><section id="block-aggregator-category-1" class="block block-aggregator even last block-count-7 block-region-tertiary-content block-category-1"  role="complementary"><div class="block-inner clearfix">  
      <h2 class="block-title">Elsewhere</h2>
    <h3 class="block_subtitle"><em>Writing</em> from my <em>other sites</em>.</h3>
  <div class="block-content content"><div class="item-list"><ul><li class="even first"><a href="https://vividness.live/2019/04/24/reinventing-buddhist-tantra-annotated-table-of-contents/">Reinventing Buddhist Tantra: Annotated Table of Contents</a>
</li><li class="odd last"><a href="https://vividness.live/2019/03/27/ann-gleig-american-dharma/">Enlightenments beyond the Enlightenment</a>
</li></ul></div></div>
</div></section></div></div>        </div>
      </div>
    
    
  </div>
</div>
  <script>ctSetCookie("apbct_check_js", "ca503bb586c2363e0c075da616a3c70d", "0");</script>-->
</body>
</html>
